{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE PIPELINE TO PROCESS FEATURES E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df178741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from feature_engineering import create_molecular_pipeline\n",
    "from transformations import LogTargetTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from main_pipeline import create_molecular_prediction_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularPredictor:\n",
    "    \"\"\"\n",
    "    Class for training and making predictions with molecular property models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasets_config, output_dir=\"models\", preprocessing_options=None):\n",
    "        \"\"\"\n",
    "        Initialize the MolecularPredictor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        datasets_config : dict\n",
    "            Configuration for datasets (if None, uses default configs)\n",
    "        output_dir : str\n",
    "            Directory to save models and results\n",
    "        preprocessing_options : dict or None\n",
    "            Additional preprocessing options (if None, uses defaults)\n",
    "        \"\"\"\n",
    "        self.datasets_config = datasets_config\n",
    "        # Default preprocessing options\n",
    "        self.preprocessing_options = {\n",
    "            'drop_missing_target': True,\n",
    "            'drop_missing_smiles': True,\n",
    "            'drop_invalid_smiles': True,\n",
    "            'convert_target_to_numeric': True\n",
    "        }\n",
    "\n",
    "        # Update with custom options if provided\n",
    "        if preprocessing_options is not None:\n",
    "            self.preprocessing_options.update(preprocessing_options)\n",
    "            \n",
    "        self.output_dir = output_dir\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "    \n",
    "    def _preprocess_data(self, data, smiles_col, target_col, verbose=1):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset by cleaning target variable and handling missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pandas.DataFrame\n",
    "            The dataset to preprocess\n",
    "        smiles_col : str\n",
    "            Name of the SMILES column\n",
    "        target_col : str\n",
    "            Name of the target column\n",
    "        verbose : int\n",
    "            Verbosity level\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Preprocessed dataset\n",
    "        \"\"\"\n",
    "        original_row_count = len(data)\n",
    "        \n",
    "        # Make a copy to avoid modifying the original\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Check for missing SMILES\n",
    "        missing_smiles = data[smiles_col].isna().sum()\n",
    "        if missing_smiles > 0:\n",
    "            if verbose >= 1:\n",
    "                print(f\"Found {missing_smiles} rows with missing SMILES. Dropping these rows.\")\n",
    "            data = data.dropna(subset=[smiles_col])\n",
    "        \n",
    "        # Convert SMILES to string type if not already\n",
    "        data[smiles_col] = data[smiles_col].astype(str)\n",
    "        \n",
    "        # Check for invalid SMILES (completely empty or just whitespace)\n",
    "        invalid_smiles = data[data[smiles_col].str.strip() == \"\"].shape[0]\n",
    "        if invalid_smiles > 0:\n",
    "            if verbose >= 1:\n",
    "                print(f\"Found {invalid_smiles} rows with empty SMILES. Dropping these rows.\")\n",
    "            data = data[data[smiles_col].str.strip() != \"\"]\n",
    "        \n",
    "        # Handle target variable\n",
    "        if target_col in data.columns:\n",
    "            # Try to convert target to numeric, coercing errors to NaN\n",
    "            original_target_type = data[target_col].dtype\n",
    "            data[target_col] = pd.to_numeric(data[target_col], errors='coerce')\n",
    "            \n",
    "            # Count how many conversions failed (became NaN)\n",
    "            conversion_failures = data[target_col].isna().sum()\n",
    "            if conversion_failures > 0 and verbose >= 1:\n",
    "                print(f\"Converted target from {original_target_type} to numeric. \"\n",
    "                      f\"{conversion_failures} values couldn't be converted and became NaN.\")\n",
    "            \n",
    "            # Drop rows with missing target values\n",
    "            missing_target = data[target_col].isna().sum()\n",
    "            if missing_target > 0:\n",
    "                if verbose >= 1:\n",
    "                    print(f\"Dropping {missing_target} rows with missing target values.\")\n",
    "                data = data.dropna(subset=[target_col])\n",
    "        \n",
    "        # Report total rows removed\n",
    "        final_row_count = len(data)\n",
    "        rows_removed = original_row_count - final_row_count\n",
    "        if rows_removed > 0 and verbose >= 1:\n",
    "            print(f\"Preprocessing removed {rows_removed} rows ({rows_removed/original_row_count:.1%}). \"\n",
    "                  f\"{final_row_count} rows remaining.\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def fit(self, dataset_name=None, custom_param_grid=None, test_size=0.2, cv=5, verbose=1):\n",
    "        \"\"\"\n",
    "        Train models for the specified dataset(s) with hyperparameter optimization\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset_name : str or None\n",
    "            Name of the dataset to train on (if None, trains on all datasets)\n",
    "        custom_param_grid : dict\n",
    "            Custom parameter grid for grid search (if None, uses default grid)\n",
    "        test_size : float\n",
    "            Proportion of data to use for testing\n",
    "        cv : int\n",
    "            Number of cross-validation folds\n",
    "        verbose : int\n",
    "            Verbosity level (0-3)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : MolecularPredictor\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        # Default parameter grid if none provided\n",
    "        if custom_param_grid is None:\n",
    "            param_grid = {\n",
    "                # Feature generation parameters\n",
    "                'features__features__fingerprint_pipe__fingerprints__radius': [2, 3],\n",
    "                'features__features__fingerprint_pipe__fingerprints__n_bits': [1024, 2048],\n",
    "                \n",
    "                # Model parameters\n",
    "                'regressor__regressor__n_estimators': [100, 200, 300, 400, 500],\n",
    "                'regressor__regressor__max_depth': [i for i in np.arange(5, 51, 2)] + [None],\n",
    "            }\n",
    "        else:\n",
    "            param_grid = custom_param_grid\n",
    "\n",
    "        # Determine which datasets to train on\n",
    "        datasets_to_train = [dataset_name] if dataset_name else self.datasets_config.keys()\n",
    "\n",
    "        for ds_name in datasets_to_train:\n",
    "            if ds_name not in self.datasets_config:\n",
    "                raise ValueError(f\"Unknown dataset: {ds_name}\")\n",
    "            \n",
    "            if verbose >= 1:\n",
    "                print(f\"\\nTraining model for dataset: {ds_name}\")\n",
    "            \n",
    "            # Load dataset\n",
    "            config = self.datasets_config[ds_name]\n",
    "            data = pd.read_csv(config[\"file\"])\n",
    "            smiles_col = config[\"smiles_col\"]\n",
    "            target_col = config[\"target_col\"]\n",
    "            \n",
    "            # Preprocess the data\n",
    "            data = self._preprocess_data(data, smiles_col, target_col, verbose)\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                data[smiles_col], \n",
    "                data[target_col],\n",
    "                test_size=test_size,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Create molecular prediction pipeline\n",
    "            pipeline = create_molecular_prediction_pipeline(\n",
    "                regressor=RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "                log_transform_target=True,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Grid search with cross-validation\n",
    "            if verbose >= 1:\n",
    "                print(f\"Running grid search with {cv}-fold cross-validation...\")\n",
    "                \n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline,\n",
    "                param_grid=param_grid,\n",
    "                cv=cv,\n",
    "                scoring='neg_root_mean_squared_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=max(0, verbose-1)\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Get best model\n",
    "            best_model = grid_search.best_estimator_\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            self.results[ds_name] = {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'test_rmse': rmse,\n",
    "                'test_r2': r2,\n",
    "                'cv_results': grid_search.cv_results_\n",
    "            }\n",
    "\n",
    "            # Store model\n",
    "            self.models[ds_name] = best_model\n",
    "            \n",
    "            # Save model to disk\n",
    "            model_path = os.path.join(self.output_dir, config[\"model_file\"])\n",
    "            joblib.dump(best_model, model_path)\n",
    "\n",
    "            # Save detailed results\n",
    "            results_path = os.path.join(self.output_dir, f\"{ds_name}_results.txt\")\n",
    "            with open(results_path, 'w') as f:\n",
    "                f.write(f\"Dataset: {ds_name}\\n\")\n",
    "                f.write(f\"Best Parameters: {grid_search.best_params_}\\n\")\n",
    "                f.write(f\"Test RMSE: {rmse:.4f}\\n\")\n",
    "                f.write(f\"Test R²: {r2:.4f}\\n\\n\")\n",
    "                f.write(\"Top Grid Search CV Results:\\n\")\n",
    "                \n",
    "                # Get indices sorted by score (negative because higher is better for neg_rmse)\n",
    "                sorted_indices = np.argsort(-grid_search.cv_results_['mean_test_score'])\n",
    "                for i in sorted_indices[:10]:  # Top 10 configurations\n",
    "                    mean = -grid_search.cv_results_['mean_test_score'][i]  # Convert back to RMSE\n",
    "                    std = grid_search.cv_results_['std_test_score'][i]\n",
    "                    f.write(f\"Configuration: Mean RMSE={mean:.4f}, Std={std:.4f}\\n\")\n",
    "                    for param, value in grid_search.cv_results_['params'][i].items():\n",
    "                        f.write(f\"  {param}: {value}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            if verbose >= 1:\n",
    "                print(f\"Model for {ds_name} trained and saved.\")\n",
    "                print(f\"Test RMSE: {rmse:.4f}, Test R²: {r2:.4f}\")\n",
    "                print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        # Print summary if training multiple datasets\n",
    "        if verbose >= 1 and len(datasets_to_train) > 1:\n",
    "            print(\"\\nTraining Summary:\")\n",
    "            for ds_name in datasets_to_train:\n",
    "                result = self.results[ds_name]\n",
    "                print(f\"{ds_name}: RMSE={result['test_rmse']:.4f}, R²={result['test_r2']:.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, smiles, dataset_name):\n",
    "        \"\"\"\n",
    "        Make predictions using a trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        smiles : str or list\n",
    "            SMILES string(s) to predict\n",
    "        dataset_name : str\n",
    "            Name of the dataset/model to use for prediction\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        if dataset_name not in self.datasets_config:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "        \n",
    "        # Convert single SMILES to list if needed\n",
    "        if isinstance(smiles, str):\n",
    "            smiles = [smiles]\n",
    "        \n",
    "        # Clean SMILES input - handle None values or empty strings\n",
    "        cleaned_smiles = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for i, s in enumerate(smiles):\n",
    "            if s is not None and str(s).strip():\n",
    "                cleaned_smiles.append(str(s).strip())\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        if len(cleaned_smiles) == 0:\n",
    "            return np.array([])\n",
    "            \n",
    "        # Check if model is already loaded\n",
    "        if dataset_name in self.models:\n",
    "            model = self.models[dataset_name]\n",
    "        else:\n",
    "            # Try to load model from disk\n",
    "            model_path = os.path.join(self.output_dir, self.datasets_config[dataset_name][\"model_file\"])\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model file not found: {model_path}. Please train the model first.\")\n",
    "            \n",
    "            model = joblib.load(model_path)\n",
    "            self.models[dataset_name] = model\n",
    "\n",
    "        # Make predictions for valid SMILES\n",
    "        predictions = model.predict(cleaned_smiles)\n",
    "        \n",
    "        # If any SMILES were invalid, create a result array with NaNs for invalid entries\n",
    "        if len(valid_indices) < len(smiles):\n",
    "            full_predictions = np.full(len(smiles), np.nan)\n",
    "            for i, valid_idx in enumerate(valid_indices):\n",
    "                full_predictions[valid_idx] = predictions[i]\n",
    "            return full_predictions\n",
    "        else:\n",
    "            return predictions\n",
    "    \n",
    "    def predict_to_csv(self, smiles, dataset_name, output_file=None):\n",
    "        \"\"\"\n",
    "        Make predictions and save to CSV file\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        smiles : str or list\n",
    "            SMILES string(s) to predict\n",
    "        dataset_name : str\n",
    "            Name of the dataset/model to use for prediction\n",
    "        output_file : str or None\n",
    "            Output file path (if None, generates a default name)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with SMILES and predictions\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        predictions = self.predict(smiles, dataset_name)\n",
    "        \n",
    "        # Ensure smiles is a list\n",
    "        if isinstance(smiles, str):\n",
    "            smiles = [smiles]\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results_df = pd.DataFrame({\n",
    "            'SMILES': smiles,\n",
    "            'Prediction': predictions\n",
    "        })\n",
    "        \n",
    "        # Save to CSV if output file provided\n",
    "        if output_file is None:\n",
    "            output_file = os.path.join(self.output_dir, f\"predictions_{dataset_name}.csv\")\n",
    "            \n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def get_results(self, dataset_name=None):\n",
    "        \"\"\"\n",
    "        Get training results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset_name : str or None\n",
    "            Name of the dataset (if None, returns all results)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary of results\n",
    "        \"\"\"\n",
    "        if dataset_name is None:\n",
    "            return self.results\n",
    "        elif dataset_name in self.results:\n",
    "            return self.results[dataset_name]\n",
    "        else:\n",
    "            raise ValueError(f\"No results found for dataset: {dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1bffe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom configuration for your datasets\n",
    "datasets_config = {\n",
    "    \"dataset1\": {\n",
    "        \"file\": \"training_data/GCGR.csv\",\n",
    "        \"smiles_col\": \"Ligand SMILES\",\n",
    "        \"target_col\": \"EC50 (nM)\",\n",
    "        \"model_file\": \"model_gcgr\"\n",
    "    },\n",
    "    \"dataset2\": {\n",
    "        \"file\": \"training_data/GIP.csv\", \n",
    "        \"smiles_col\": \"Ligand SMILES\",\n",
    "        \"target_col\": \"EC50 (nM)\",\n",
    "        \"model_file\": \"model_gip\"\n",
    "    },\n",
    "    \"dataset3\": {\n",
    "        \"file\": \"training_data/GLP-1R.csv\",\n",
    "        \"smiles_col\": \"Ligand SMILES\", \n",
    "        \"target_col\": \"EC50 (nM)\",\n",
    "        \"model_file\": \"model_glp1r\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be0eaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MolecularPredictor(\n",
    "        datasets_config=datasets_config,\n",
    "        output_dir=\"trained_models\",\n",
    "        preprocessing_options={\n",
    "            'drop_missing_target': True,  #  drop rows with missing target\n",
    "            'drop_missing_smiles': True,  #  drop rows with missing SMILES\n",
    "            'drop_invalid_smiles': True,  # drop rows with empty/invalid SMILES\n",
    "            'convert_target_to_numeric': True  # convert target to numeric\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20c60f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for dataset: dataset1\n",
      "Converted target from object to numeric. 1910 values couldn't be converted and became NaN.\n",
      "Dropping 1910 rows with missing target values.\n",
      "Preprocessing removed 1910 rows (91.4%). 179 rows remaining.\n",
      "Running grid search with 5-fold cross-validation...\n",
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/decision/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/externals/loky/backend/queues.py\", line 159, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n  File \"/home/decision/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 215, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"/home/decision/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/externals/loky/backend/reduction.py\", line 208, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"/home/decision/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n    return Pickler.dump(self, obj)\nTypeError: cannot pickle 'Boost.Python.function' object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 194\u001b[0m, in \u001b[0;36mMolecularPredictor.fit\u001b[0;34m(self, dataset_name, custom_param_grid, test_size, cv, verbose)\u001b[0m\n\u001b[1;32m    184\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m    185\u001b[0m     pipeline,\n\u001b[1;32m    186\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, verbose\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Get best model\u001b[39;00m\n\u001b[1;32m    197\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/parallel.py:1691\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1685\u001b[0m \n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1692\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/parallel.py:1726\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/parallel.py:735\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    729\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/oam_iitk/lib/python3.9/site-packages/joblib/parallel.py:753\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 753\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "predictor.fit(dataset_name= 'dataset1',verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626074f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71522878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca6f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629d211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b615cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c9165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab446e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oam_iitk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
